{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNkPDzMNs29iFCsegwjxTAC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"62a0d66b6fa8484bb750edb8f58307ef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_113c5832551947a181bc5515afcbf568","IPY_MODEL_630e7052a7d5497dbaa6ffb7ea855ed5","IPY_MODEL_42d30e84609c4cc4926a078ae7552321"],"layout":"IPY_MODEL_7a1f85c4c260404f944902a5a4ed2dab"}},"113c5832551947a181bc5515afcbf568":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aff9c7c17151407bb69e806cf46bf34c","placeholder":"​","style":"IPY_MODEL_c72134211e9f48189e611a6ca872f9bf","value":"100%"}},"630e7052a7d5497dbaa6ffb7ea855ed5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bac422bbf75c4e85b67c731789104160","max":102530333,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9935da7de627412987002abe85280a89","value":102530333}},"42d30e84609c4cc4926a078ae7552321":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_017c56259916480996c5177171dc6da1","placeholder":"​","style":"IPY_MODEL_fd9317903d0941048bd062cda3431390","value":" 97.8M/97.8M [00:00&lt;00:00, 198MB/s]"}},"7a1f85c4c260404f944902a5a4ed2dab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aff9c7c17151407bb69e806cf46bf34c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c72134211e9f48189e611a6ca872f9bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bac422bbf75c4e85b67c731789104160":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9935da7de627412987002abe85280a89":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"017c56259916480996c5177171dc6da1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd9317903d0941048bd062cda3431390":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["%%capture\n","!wget https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\n","!tar -xzvf images.tar.gz\n","\n","!wget https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz\n","!tar -xzvf annotations.tar.gz"],"metadata":{"id":"7sSK-GJSYOmm","executionInfo":{"status":"ok","timestamp":1676355977930,"user_tz":-540,"elapsed":22391,"user":{"displayName":"김지환","userId":"06313252523655750523"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## Data load"],"metadata":{"id":"Q4xr-AzDdyK7"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","import xml.etree.ElementTree as ET\n","import os\n","import collections\n","from PIL import Image\n","import numpy as np\n","\n","class PetDataset(Dataset):\n","\n","\tdef __init__(self, transform):\n","\t\tself.class_idx = {}\n","\t\tself.idx_class = {}\n","\t\tself.transform = transform\n","\t\tself.data = self.load_data()\t\t\n","\n","  \t# 총 데이터의 개수를 리턴\n","\tdef __len__(self): \n","\t\treturn len(self.data)\n","\n","  \t# 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n","\tdef __getitem__(self, idx): \n","\t\timgpath, bbox, label = self.data[idx]\n","\t\timg = Image.open(imgpath).convert('RGB')\n","\t\tif self.transform:\n","\t\t\timg = self.transform(img)\n","\t\tsample = (img, bbox, label)\n","\t\treturn sample\n","\n","\tdef parse_voc_xml(self, node): # xml 파일을 dictionary로 반환\n","\t\tvoc_dict = {}\n","\t\tchildren = list(node)\n","\t\tif children:\n","\t\t\tdef_dic: Dict[str, Any] = collections.defaultdict(list)\n","\t\t\tfor dc in map(self.parse_voc_xml, children):\n","\t\t\t\tfor ind, v in dc.items():\n","\t\t\t\t\tdef_dic[ind].append(v)\n","\t\t\tif node.tag == \"annotation\":\n","\t\t\t\tdef_dic[\"object\"] = [def_dic[\"object\"]]\n","\t\t\tvoc_dict = {node.tag: {ind: v[0] if len(v) == 1 else v for ind, v in def_dic.items()}}\n","\t\tif node.text:\n","\t\t\ttext = node.text.strip()\n","\t\t\tif not children:\n","\t\t\t\tvoc_dict[node.tag] = text\n","\t\treturn voc_dict\n","\t\n","\tdef load_data(self):\n","\t\txmlpath = '/content/annotations/xmls'\n","\t\timgpath = '/content/images'\n","\t\tdata = []\n","\t\tfor xmlfile in os.listdir(xmlpath):\n","\t\t\tnode = ET.parse(os.path.join(xmlpath, xmlfile)).getroot()\n","\t\t\txmls = self.parse_voc_xml(node)\n","\t\t\timgfile = os.path.join(imgpath, xmls['annotation']['filename'])\n","\t\t\tdogobject = xmls['annotation']['object'][0]\n","\t\t\tbbox = np.array([int(dogobject['bndbox']['xmin']), int(dogobject['bndbox']['ymin']), int(dogobject['bndbox']['xmax']), int(dogobject['bndbox']['ymax'])], dtype=np.float32)\n","\t\t\tclass_name = xmls['annotation']['filename'].split('_')[0]\n","\t\t\tif class_name not in self.class_idx:\n","\t\t\t\tclassidx = len(class_name)\n","\t\t\t\tself.class_idx[class_name] = classidx\n","\t\t\t\tself.idx_class[classidx] = class_name\n","\t\t\telse:\n","\t\t\t\tclassidx = self.class_idx[class_name]\n","\t\t\tdata.append((imgfile, bbox, classidx))\n","\t\treturn data\n","\n","if __name__==\"__main__\":\n","\timport torch\n","\tfrom torchvision import transforms\n","\ttransform = transforms.Compose([transforms.ToTensor(), \n","\t\t\t\t\t\t\ttransforms.Resize((224, 224)),\n","\t\t\t\t\t\t\ttransforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","\t\t\t\t\t\t\t])\n","\tdataset = PetDataset(transform=transform)\n","\ttrain_set , val_set = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), len(dataset)-int(len(dataset)*0.8)])\n","\t\n","\ttrain_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n","\tval_loader = torch.utils.data.DataLoader(dataset,batch_size=32, shuffle=False) \n","\n","\tbbox = (next(iter(train_loader)))[1]\n","\tprint(bbox)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ggHq1cQVdyip","executionInfo":{"status":"ok","timestamp":1676355981042,"user_tz":-540,"elapsed":3149,"user":{"displayName":"김지환","userId":"06313252523655750523"}},"outputId":"84b34177-29b8-4f86-93f4-b83917e949a5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 87.,  39., 319., 356.],\n","        [214., 148., 300., 243.]])\n"]}]},{"cell_type":"markdown","source":["## Model"],"metadata":{"id":"cdZIqDejkJUM"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"xIube732XDzL","executionInfo":{"status":"ok","timestamp":1676355981042,"user_tz":-540,"elapsed":7,"user":{"displayName":"김지환","userId":"06313252523655750523"}}},"outputs":[],"source":["# import the necessary packages\n","from torch.nn import Dropout\n","from torch.nn import Identity\n","from torch.nn import Linear\n","from torch.nn import Module\n","from torch.nn import ReLU\n","from torch.nn import Sequential\n","from torch.nn import Sigmoid\n","\n","class ObjectDetector(Module):\n","\tdef __init__(self, baseModel, numClasses):\n","\t\tsuper(ObjectDetector, self).__init__()\n","\t\t# initialize the base model and the number of classes\n","\t\tself.baseModel = baseModel\n","\t\tself.numClasses = numClasses\n","\t    # build the regressor head for outputting the bounding box\n","\t\t# coordinates\n","\t\tself.regressor = Sequential(\n","\t\t\tLinear(baseModel.fc.in_features, 128),\n","\t\t\tReLU(),\n","\t\t\tLinear(128, 64),\n","\t\t\tReLU(),\n","\t\t\tLinear(64, 32),\n","\t\t\tReLU(),\n","\t\t\tLinear(32, 4),\n","\t\t\tSigmoid()\n","\t\t)\n","\t    # build the classifier head to predict the class labels\n","\t\tself.classifier = Sequential(\n","\t\t\tLinear(baseModel.fc.in_features, 512),\n","\t\t\tReLU(),\n","\t\t\tDropout(),\n","\t\t\tLinear(512, 512),\n","\t\t\tReLU(),\n","\t\t\tDropout(),\n","\t\t\tLinear(512, self.numClasses)\n","\t\t)\n","\t\t# set the classifier of our base model to produce outputs\n","\t\t# from the last convolution block\n","\t\tself.baseModel.fc = Identity()\n","  \n","\tdef forward(self, x):\n","\t\t# pass the inputs through the base model and then obtain\n","\t\t# predictions from two different branches of the network\n","\t\tfeatures = self.baseModel(x)\n","\t\tbboxes = self.regressor(features)\n","\t\tclassLogits = self.classifier(features)\n","\t\t# return the outputs as a tuple\n","\t\treturn (bboxes, classLogits)"]},{"cell_type":"markdown","source":["## Train"],"metadata":{"id":"AJuNMaf6kM3w"}},{"cell_type":"code","source":["import torch\n","from torchvision import transforms\n","# from dataload import PetDataset\n","# from model import ObjectDetector\n","import tqdm\n","from torchvision.models import resnet50\n","from torch.nn import CrossEntropyLoss\n","from torch.nn import MSELoss, L1Loss\n","from torch.optim import Adam\n","import numpy as np\n","\n","def train_one_epoch(train_loader, detector):\n","\t# set the model in training mode\n","\tdetector.train()\n","\t# initialize the total training and validation loss\n","\ttotalTrainLoss = 0\n","\t# initialize the number of correct predictions in the training\n","\t# and validation step\n","\ttrainCorrect = 0\n","\n","\tclassLossFunc = CrossEntropyLoss()\n","\tbboxLossFunc = L1Loss()\n","\topt = Adam(detector.parameters(), lr=1e-4)\n","\n","\tfor imgs, bboxes, labels in train_loader:\n","\t\timgs = imgs.to(\"cuda\")\n","\t\tbboxes = bboxes.to(\"cuda\")\n","\t\tlabels = labels.to(\"cuda\")\n","\n","\t\t# perform a forward pass and calculate the training loss\n","\t\tpredictions = detector(imgs)\n","\n","\t\tbboxLoss = bboxLossFunc(predictions[0], bboxes)\n","\t\tclassLoss = classLossFunc(predictions[1], labels)\t\t\t\t\t\n","\t\ttotalLoss = (0.1 * bboxLoss) + (0.9 * classLoss)\n","\n","\t\t# zero out the gradients, perform the backpropagation step,\n","\t\t# and update the weights\n","\t\topt.zero_grad()\n","\t\ttotalLoss.backward()\n","\t\topt.step()\n","\n","\t\t# add the loss to the total training loss so far and\n","\t\t# calculate the number of correct predictions\n","\t\ttotalTrainLoss += totalLoss\n","\t\ttrainCorrect += (predictions[1].argmax(1) == labels).type(\n","\t\t\ttorch.float).sum().item()\n","\n","\treturn totalTrainLoss, trainCorrect\n","\n","def eval(val_loader, detector):\n","\tclassLossFunc = CrossEntropyLoss()\n","\tbboxLossFunc = L1Loss()\n","\n","\t# switch off autograd\n","\twith torch.no_grad():\n","\t\tdetector.eval()\n","\t\t# initialize the total training and validation loss\n","\t\ttotalValLoss = 0\n","\t\t# initialize the number of correct predictions in the training\n","\t\t# and validation step\n","\t\tvalCorrect = 0\n","\n","\t\tfor imgs, bboxes, labels in val_loader:        \n","\t\t\timgs = imgs.to(\"cuda\")\n","\t\t\tbboxes = bboxes.to(\"cuda\")\n","\t\t\tlabels = labels.to(\"cuda\")\n","\n","\t\t\t# perform a forward pass and calculate the training loss\n","\t\t\tpredictions = detector(imgs)\n","\t\t\tbboxLoss = bboxLossFunc(predictions[0], bboxes)\n","\t\t\tclassLoss = classLossFunc(predictions[1], labels)\n","\t\t\ttotalLoss = (0.1 * bboxLoss) + (0.9 * classLoss)\n","\n","\t\t\t# add the loss to the total training loss so far and\n","\t\t\t# calculate the number of correct predictions\n","\t\t\ttotalValLoss += totalLoss\n","\t\t\tvalCorrect += (predictions[1].argmax(1) == labels).type(\n","\t\t\ttorch.float).sum().item()\n","\t\treturn totalValLoss, valCorrect\n","\n","def train(epoch):\n","\ttransform = transforms.Compose([transforms.ToTensor(), \n","\t\t\t\t\t\t\ttransforms.Resize((224, 224)),\n","\t\t\t\t\t\t\ttransforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","\t\t\t\t\t\t\t])\n","\tdataset = PetDataset(transform=transform)\n","\ttrain_set , val_set = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), len(dataset)-int(len(dataset)*0.8)])\n","\t\n","\ttrain_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n","\tval_loader = torch.utils.data.DataLoader(dataset,batch_size=32, shuffle=False) \n","\n","\n","\tresnet = resnet50(pretrained=True)\n","\n","\t# freeze all ResNet50 layers so they will *not* be updated during the\n","\t# training process\n","\tfor param in resnet.parameters():\n","\t\tparam.requires_grad = False\n","\n","\tnum_classes = len(dataset.class_idx)\n","\tobjectDetector = ObjectDetector(resnet, num_classes)\n","\tobjectDetector = objectDetector.to(\"cuda\")\n","\tprint(objectDetector)\n","\n","\n","\tprint(\"[INFO] training the network...\")\n","\tfor e in tqdm.tqdm(range(epoch)):\n","\t\ttotalTrainLoss, trainCorrect = train_one_epoch(train_loader, objectDetector)\n","\t\tprint(\"Train loss\", float(totalTrainLoss), \"Correct\", trainCorrect/len(train_loader))\n","\t\ttotalValLoss, valCorrect = eval(val_loader, objectDetector)\n","\t\tprint(\"Val loss\", float(totalValLoss), \"Correct\", valCorrect/len(val_loader))\n","\n","if __name__==\"__main__\":\n","\ttrain(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["62a0d66b6fa8484bb750edb8f58307ef","113c5832551947a181bc5515afcbf568","630e7052a7d5497dbaa6ffb7ea855ed5","42d30e84609c4cc4926a078ae7552321","7a1f85c4c260404f944902a5a4ed2dab","aff9c7c17151407bb69e806cf46bf34c","c72134211e9f48189e611a6ca872f9bf","bac422bbf75c4e85b67c731789104160","9935da7de627412987002abe85280a89","017c56259916480996c5177171dc6da1","fd9317903d0941048bd062cda3431390"]},"id":"cuTrEzB4cntt","executionInfo":{"status":"ok","timestamp":1676356186612,"user_tz":-540,"elapsed":205575,"user":{"displayName":"김지환","userId":"06313252523655750523"}},"outputId":"94626f5d-95b9-4894-a6e5-e2b1462a712d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/97.8M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62a0d66b6fa8484bb750edb8f58307ef"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ObjectDetector(\n","  (baseModel): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (4): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (5): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Identity()\n","  )\n","  (regressor): Sequential(\n","    (0): Linear(in_features=2048, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=128, out_features=64, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=64, out_features=32, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=32, out_features=4, bias=True)\n","    (7): Sigmoid()\n","  )\n","  (classifier): Sequential(\n","    (0): Linear(in_features=2048, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=512, out_features=512, bias=True)\n","    (4): ReLU()\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=512, out_features=35, bias=True)\n","  )\n",")\n","[INFO] training the network...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/3 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Train loss 4496.38232421875 Correct 4.822510822510822\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 1/3 [01:10<02:20, 70.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Val loss 2193.388427734375 Correct 16.017241379310345\n","Train loss 4347.28369140625 Correct 8.796536796536797\n"]},{"output_type":"stream","name":"stderr","text":["\r 67%|██████▋   | 2/3 [02:14<01:06, 66.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Val loss 2136.343994140625 Correct 22.370689655172413\n","Train loss 4279.10986328125 Correct 10.38961038961039\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [03:18<00:00, 66.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Val loss 2109.836669921875 Correct 25.70689655172414\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"n9lgAkSfj02a"},"execution_count":null,"outputs":[]}]}